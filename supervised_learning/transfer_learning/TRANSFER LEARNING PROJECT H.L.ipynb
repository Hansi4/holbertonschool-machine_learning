{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c549e114",
   "metadata": {},
   "source": [
    "# Transfer Learning Applications on Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719db2ff",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee758a92",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset is a widely used benchmark for image classification tasks. It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The classes include airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
    "\n",
    "The images are low-resolution and can be challenging to classify due to their small size and variability in appearance. The dataset is commonly used to evaluate the performance of image classification algorithms, including deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553489bc",
   "metadata": {},
   "source": [
    "## Setting up: Loading CIFAR-10\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The dataset is split into two subsets:\n",
    "\n",
    "a training set of 50,000 images and\n",
    "a testing set of 10,000 images\n",
    "\n",
    "The libraries loaded, including TensorFlow and Keras, provide essential tools and functions for building and training neural network models efficiently. Specifically, TensorFlow serves as the foundational framework for constructing computational graphs and executing machine learning operations, while Keras offers a high-level API that simplifies the process of building and training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TensorFlow library, which is used for building and training machine learning models\n",
    "import tensorflow as tf  \n",
    "\n",
    "# Importing the CIFAR-10 dataset from Keras library, which is a collection of images commonly used for training machine learning\n",
    "from keras.datasets import cifar10  \n",
    "\n",
    "# Importing utility to convert class vectors to binary class matrices (one-hot encoding)\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Importing the preprocess_input function specific to ResNet50 to preprocess image data\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Importing necessary components from Keras to build and customize the model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Flatten, UpSampling2D, Dropout\n",
    "\n",
    "# Importing optimization algorithms from Keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# Importing callback function for early stopping during training to prevent overfitting\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b00618",
   "metadata": {},
   "source": [
    "The subsequent step involves loading the CIFAR-10 dataset using Keras' built-in cifar10.load_data() function. This dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The images are split into 50,000 training samples and 10,000 test samples. After loading the dataset, the pixel values are convereted to floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras library, which provides a high-level neural networks API, running on top of TensorFlow\n",
    "import keras\n",
    "\n",
    "# Load the CIFAR-10 dataset, x_train_full and x_test contain the image data, while y_train_full and y_test contain the labels\n",
    "(x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Print the shapes of the training and test image datasets to verify their dimensions\n",
    "print(f\"Training data shape: {x_train_full.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library, which is used for numerical operations on arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# Import the Matplotlib library, specifically the pyplot module, which is used for creating visualizations and plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the sixth image in the training dataset using Matplotlib's imshow function\n",
    "plt.imshow(x_train_full[6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1125f",
   "metadata": {},
   "source": [
    "## Visualizing the dataset\n",
    "\n",
    "The Cifar-10 dataset contains 10 classes\n",
    "\n",
    "These classes can be challenging to classify due to several factors, including: \n",
    "\n",
    "the variability in appearance within each class\n",
    "\n",
    "the presence of similar-looking objects across different classes\n",
    "\n",
    "variations in orientation, lighting conditions, and background clutter\n",
    "\n",
    "relatively low resolution (32x32 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Visualize random images from the dataset\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(x_train_full))\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_train_full[idx])\n",
    "    plt.title(class_names[y_train_full[idx][0]])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14cfa1",
   "metadata": {},
   "source": [
    "## Preprocessing Input Data for Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabe365",
   "metadata": {},
   "source": [
    "In this step, we preprocess the input data using the preprocess_input function. This function is essential when working with pre-trained convolutional neural network (CNN) models like ResNet50, VGG16, or InceptionV3. Preprocessing ensures that our input images are formatted correctly and normalized before being fed into the neural network for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0ac8c",
   "metadata": {},
   "source": [
    "Key functionalities of preprocess_input:\n",
    "\n",
    "Standardizes input data to a format expected by pre-trained CNN models.\n",
    "\n",
    "Typically performs mean normalization and channel-wise color normalization.\n",
    "\n",
    "Scales and centers input images appropriately, aiding in better convergence during training and improved accuracy during inference.\n",
    "\n",
    "Helps maintain numerical stability, preventing issues like vanishing or exploding gradients.\n",
    "\n",
    "Enhances the model's generalization capabilities by ensuring consistent and standardized input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data and the test data to float32 type for compatibility with model input requirements\n",
    "x_train_full = x_train_full.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Apply the preprocess_input function from ResNet50 to the training data and the test data for preprocessing\n",
    "x_train_full = preprocess_input(x_train_full)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "# Print the shape of the training data and the test data to confirm dimensions and preprocessing\n",
    "print(f\"Training data shape: {x_train_full.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b6e6f",
   "metadata": {},
   "source": [
    "## Splitting and Categorizing Training and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12dc20",
   "metadata": {},
   "source": [
    "In this step, we divide the training set into separate training and validation sets using slicing operations. The training set, x_train_full, is split into two parts:\n",
    "\n",
    "x_train containing the majority of the data\n",
    "\n",
    "x_valid containing a smaller portion (5000 samples in this case) which will be used for validation during model training.\n",
    "\n",
    "Similarly, the corresponding labels are also split into y_train and y_valid.\n",
    "\n",
    "Additionally, we convert the class labels from integer format to categorical format using the to_categorical function. This is necessary for categorical classification tasks like CIFAR-10, where each image is assigned one of ten possible categories.\n",
    "\n",
    "Converting the labels to categorical format ensures that they are represented as one-hot vectors, which is required by the model during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into training and validation sets\n",
    "x_train, x_valid = x_train_full[:-5000], x_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "# Convert the class labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_valid = to_categorical(y_valid, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation data shape: {x_valid.shape}, {y_valid.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffac578",
   "metadata": {},
   "source": [
    "## Defining Feature Extraction with ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59efc0",
   "metadata": {},
   "source": [
    "In this step, we define a feature extractor using the ResNet50 model, a pre-trained convolutional neural network (CNN) architecture. The purpose of a feature extractor is to leverage the learned representations from a pre-trained model to extract relevant features from input images. Here, we use ResNet50, a powerful and widely used CNN model pre-trained on the ImageNet dataset, which contains millions of images across thousands of categories.\n",
    "\n",
    "The feature_extractor function takes input tensors representing images and returns the output feature maps generated by the ResNet50 model. By setting include_top=False, we exclude the fully connected layers at the top of the ResNet50 architecture, retaining only the convolutional layers. This allows us to use ResNet50 as a feature extractor while excluding its classification layers, which are specific to the ImageNet task.\n",
    "\n",
    "Additionally, we freeze the layers of the base ResNet50 model by setting layer.trainable = False for each layer. Freezing the layers prevents their weights from being updated during training, ensuring that only the weights of the additional layers we add on top of the base model will be trained.\n",
    "\n",
    "This approach is common in transfer learning scenarios, where we aim to leverage pre-trained models to extract useful features for a different task without modifying the learned representations in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e27c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model with pre-trained ImageNet weights, excluding the top fully connected layers\n",
    "def feature_extractor(inputs):\n",
    "    base_model = tf.keras.applications.resnet.ResNet50(input_shape=(224, 224, 3),\n",
    "                                                       include_top=False,\n",
    "                                                       weights='imagenet')\n",
    "    \n",
    "    # Freeze all layers in the base model to prevent them from being updated during training\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Return the output of the base model when applied to the input data\n",
    "    return base_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad71bd",
   "metadata": {},
   "source": [
    "## Defining the Classifier\n",
    "\n",
    "In this step, we define a classifier function that builds the classification layers on top of the features extracted by the ResNet50 model. The classifier is responsible for mapping the extracted features to the corresponding class probabilities for the given task.\n",
    "\n",
    "The classifier function takes the output feature maps from the feature extractor as input and adds several dense layers to perform classification. First, we apply a global average pooling layer to reduce the spatial dimensions of the feature maps while retaining important spatial information. Then, we flatten the pooled feature maps into a 1D vector to feed into the fully connected layers.\n",
    "\n",
    "Next, we add two densely connected layers with ReLU activation functions, which introduce non-linearity to the model and allow it to learn complex patterns in the data. These layers progressively reduce the dimensionality of the feature space, capturing increasingly abstract representations of the input data.\n",
    "\n",
    "Finally, we add a dense output layer with softmax activation, consisting of 10 units corresponding to the 10 classes in the CIFAR-10 dataset. The softmax function normalizes the output probabilities, ensuring that they sum up to 1 and represent the predicted probabilities for each class. The name \"classification\" is assigned to this layer for easy identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f59579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "def classifier(inputs):\n",
    "    # Global average pooling layer to reduce spatial dimensions and summarize features\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    \n",
    "    # Flatten layer to convert the 2D pooled feature map into a 1D vector\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Dense layer with 1024 units and ReLU activation function for feature processing\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    \n",
    "    # Dropout layer to prevent overfitting with 50% dropout rate\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Dense layer with 512 units and ReLU activation function for further feature processing\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    \n",
    "    # Output layer with 10 units (for 10 classes in CIFAR-10) and softmax activation for classification\n",
    "    x = Dense(10, activation=\"softmax\", name=\"classification\")(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfddeb",
   "metadata": {},
   "source": [
    "## Defining the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f31a54",
   "metadata": {},
   "source": [
    "In this step, we define the final model by integrating the feature extraction and classification components. The final model takes input tensors representing images and produces output predictions for the given task.\n",
    "\n",
    "The final_model function begins by upsampling the input images using the UpSampling2D layer. This step increases the spatial dimensions of the images to match the input size expected by the ResNet50 model. By resizing the images to a size of (224, 224), we ensure compatibility with the input shape required by the pre-trained ResNet50 architecture.\n",
    "\n",
    "Next, the resized images are passed through the feature extractor, which extracts relevant features from the input images using the pre-trained ResNet50 model. The feature extractor leverages the learned representations from the ResNet50 architecture to capture meaningful patterns and characteristics present in the images.\n",
    "\n",
    "The extracted features are then fed into the classifier, which consists of several densely connected layers followed by a softmax output layer. The classifier processes the extracted features and generates class probabilities for each input image, indicating the likelihood of belonging to each of the predefined classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a807e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final model\n",
    "def final_model(inputs):\n",
    "    # Upsampling layer to resize the input tensor\n",
    "    resize = UpSampling2D(size=(7, 7))(inputs)\n",
    "    \n",
    "    # Extract features using the feature_extractor function defined earlier\n",
    "    resnet_feature_extractor = feature_extractor(resize)\n",
    "    \n",
    "    # Classify features using the classifier function defined earlier\n",
    "    classification_output = classifier(resnet_feature_extractor)\n",
    "    \n",
    "    # Return the final classification output\n",
    "    return classification_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523244d",
   "metadata": {},
   "source": [
    "## Defining and Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70b71f",
   "metadata": {},
   "source": [
    "In this step, we define and compile the overall model architecture by integrating the feature extraction, classification, and input layers. The defined model takes input tensors of shape (32, 32, 3), representing the dimensions of the CIFAR-10 images, and produces output predictions for the given task.\n",
    "\n",
    "The define_compile_model function starts by creating an input layer with the specified shape of (32, 32, 3), indicating the dimensions of the input images. This input layer serves as the entry point for the data into the model architecture.\n",
    "\n",
    "Next, the final_model function is called to build the main body of the model, incorporating the feature extraction and classification components. The final_model function returns the output tensor representing the class predictions for the input images.\n",
    "\n",
    "Subsequently, the overall model is constructed using the Model class, specifying the input and output tensors. This step effectively combines the input layer, feature extraction, and classification layers into a single cohesive model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67956f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_compile_model():\n",
    "    # Define input layer with shape (32, 32, 3)\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    \n",
    "    # Generate classification output using final_model function\n",
    "    classification_output = final_model(inputs)\n",
    "    \n",
    "    # Create model with specified inputs and classification output\n",
    "    model = Model(inputs=inputs, outputs=classification_output)\n",
    "    \n",
    "    # Compile model with Adam optimizer, learning rate of 0.001, categorical crossentropy loss, and accuracy metric\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382afae",
   "metadata": {},
   "source": [
    "## Creating and Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf6d1b",
   "metadata": {},
   "source": [
    "In this step, we create the neural network model, which defines the model architecture and compiles it with specified optimization parameters, loss function, and evaluation metrics. Once the model is created, we use the summary method to print a concise summary of its architecture. This summary provides key information about the model's structure, including the type and shape of each layer, the number of parameters, and the output shape of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d15af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = define_compile_model()\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2feff",
   "metadata": {},
   "source": [
    "## Training the Model with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a1a5d",
   "metadata": {},
   "source": [
    "Here, we employ the early stopping technique by defining an early stopping callback, which monitors the validation loss during training and halts the training process if the validation loss does not improve for a specified number of epochs (patience). The restore_best_weights=True argument ensures that the model's weights are reverted to the configuration yielding the lowest validation loss when training concludes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model using the original training data and early stopping\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=15,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    batch_size=64,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39bc2a",
   "metadata": {},
   "source": [
    "## Visualizing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ba94f",
   "metadata": {},
   "source": [
    "The first plot displays the training and validation accuracy values over different epochs, helping to visualize the model's learning progress.\n",
    "\n",
    "The second plot illustrates the training and validation loss values across epochs, providing insight into how the model's performance changes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053efd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d93c8",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ca8bc",
   "metadata": {},
   "source": [
    "We evaluate the model's performance on a test dataset and computes the test accuracy by comparing its predictions to the true labels. The test accuracy reflects how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset and obtain the loss and accuracy\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af220c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Create a figure with 2 rows and 5 columns\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "\n",
    "# Plot images and their true/predicted labels\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        # Compute the index of the current image in the test set\n",
    "        index = 5 * i + j\n",
    "        \n",
    "        # Plot the current image\n",
    "        axs[i, j].imshow(x_test[index])\n",
    "        axs[i, j].axis('off')\n",
    "        \n",
    "        # Get the true label of the current image\n",
    "        true_label = y_test[index]\n",
    "        \n",
    "        # Get the predicted label of the current image\n",
    "        predicted_label = y_pred[index].argmax()\n",
    "        \n",
    "        # Add the true and predicted labels to the plot\n",
    "        axs[i, j].set_title(f'True: {true_label}\\nPred: {predicted_label}')\n",
    "        \n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
